apiVersion: kagenti.operator.dev/v1alpha1
kind: Component
metadata:
  name: research-agent
  namespace: kagenti-system 

spec:
  description: "A research agent for information gathering"

  agent:
    # Optional build specification for building from source
    build:
      mode: dev
      pipeline:
        parameters:
          - name: github-token-secret
            value: "github-credentials"  
          - name: SourceRepository
            value: "github.com/kagenti/agent-examples.git"
          - name: SourceRevision
            value: "main"
          - name: SourceSubfolder
            value: "acp/acp_ollama_researcher"
          - name: FullImageName
            value: "registry.cr-system.svc.cluster.local:5000/beai-research-agent:latest"

      cleanupAfterBuild: true
 
    # General component configuration
 
  deployer:
    name: "my-agent-deployment"
    namespace: kagenti-system 
    deployAfterBuild: true
    kubernetes:
      imageSpec:
        image: "beai-research-agent"
        imageTag: "latest"
        imageRegistry: "registry.cr-system.svc.cluster.local:5000"
        imagePullPolicy: "IfNotPresent"
      containerPorts:
        - name: "http"
          containerPort: 8090
          protocol: "TCP"
      servicePorts:
        - name: "http"
          port: 8008
          targetPort: 8090
          protocol: "TCP"   
      resources:
        limits:
          cpu: "1"
          memory: "2Gi"
        requests:
          cpu: "500m"
          memory: "1Gi"
      serviceType: "ClusterIP"

    env:
#      - name: LLM_MODEL
#        value: "llama3.2:3b-instruct-fp16"
#      - name: LLM_URL
#        value: "http://llm-service:11434"
      - name: HOST
        value: "0.0.0.0"  
      - name: LLM_API_BASE
        value: "http://host.docker.internal:11434/v1"
      - name: LLM_API_KEY
        value: "dummy"  
      - name: LLM_MODEL
        value: "llama3.2:3b-instruct-fp16"


 
